#!/usr/bin/env python3

import os
import sys
import numpy as np
import torch
import time
import glob
from torch.profiler import profile, record_function, ProfilerActivity
import torch.autograd.profiler_util


torch.backends.cuda.matmul.allow_tf32 = True

sys.path.append('.')
import graphnet as GNN

def usage():
    print('Usage: ./infer_profile <dataset> <input_path> <batch_size> <num_iters>')
    print('    (dataset: flag_simple, cylinder_flow, deforming_plate)')
    exit(1)

class MgnDataset(torch.utils.data.Dataset):
    def __init__(self, files, bs, sample_type, batch_type, dev, dtype):
        self.batchs = [
            GNN.collate_common([
                GNN.load_npz_common(files[i + j], sample_type)
                for j in range(bs)
            ], batch_type).todev(dev).asdtype(dtype)
            for i in range(0, len(files), bs)
        ]

    def __len__(self): return len(self.batchs)
    def __getitem__(self, i): return self.batchs[i]

if __name__ == '__main__':
    if len(sys.argv) != 5: usage()

    dataset_name = sys.argv[1]
    input_path = sys.argv[2]
    batch_size = int(sys.argv[3])
    num_iters = int(sys.argv[4])
    dev = torch.device(os.environ.get('MGN_DEV', 'cuda:0'))

    dtype = {
        'float16': torch.float16,
        'float32': torch.float32
    }[os.environ.get('MGN_DTYPE', 'float16')]

    print(f'Device: {dev}')
    print(f'Dtype: {dtype}')

    model = {
        'flag_simple': 'cloth',
        'cylinder_flow': 'incomprns',
        'deforming_plate': 'hyperel'
    }[dataset_name]

    if model == 'cloth': import cloth as M
    elif model == 'incomprns': import incomprns as M
    elif model == 'hyperel': import hyperel as M
    else: raise ValueError(f'Unknown model {model}')

    print(f'Model: {M.model_type.__name__}')
    print(f'Dataset: {dataset_name}')
    print(f'Fused Gather Concat: {GNN.USE_FUSED_GATHER_CONCAT}')
    print(f'Fused Scatter Concat: {GNN.USE_FUSED_SCATTER_CONCAT}')
    print(f'Fused LayerNorm MLP: {GNN.USE_FUSED_LN}')
    print(f'Fused MLP: {GNN.USE_FUSED_MLP}')
    print(f'Batch Size: {batch_size}')
    print(f'Num Iters: {num_iters}')

    net = M.model_type().eval().to(dev).to(dtype)
    files = list(glob.glob(f'{input_path}/*.npz'))
    ds = MgnDataset(files, batch_size, M.sample_type, M.batch_type, dev, dtype)

    with torch.no_grad():
        print('Benchmarking...')
        t0 = time.perf_counter()
        for _ in range(num_iters):
            for i in range(len(ds)):
                net.loss(ds[i])
        t1 = time.perf_counter()

    print(f'Elapsed time: {t1 - t0:.2f} seconds')
    print(f'Throughput: {num_iters * len(ds) * batch_size / (t1 - t0):.2f} samp/sec')

    with profile(
        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]
    ) as prof:
        with torch.no_grad():
            print('Profiling...')
            for _ in range(1):
                for i in range(len(ds)):
                    net.loss(ds[i])


    avg_list = torch.autograd.profiler_util.EventList([
        ka
        for ka in prof.key_averages(group_by_input_shape=False)
        if ka.device_type == torch.autograd.DeviceType.CUDA
    ])

    print(avg_list.table(
            sort_by="self_cuda_time_total",
            max_name_column_width=100,
            row_limit=100,
            top_level_events_only=False))
